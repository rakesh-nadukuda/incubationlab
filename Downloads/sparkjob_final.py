# -*- coding: utf-8 -*-
"""sixthnew.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pPJ0iu8aD7PY-d8s3kMfxpRV37CiCHG_
"""



import json
from pyspark.sql import SparkSession
from pyspark.sql.types import DecimalType,StringType
from pyspark.sql.functions import concat_ws,col,sha2
from pyspark.sql import functions as f
from pyspark.conf import SparkConf
import pyspark
import boto3

spark=SparkSession.builder.appName('myapp').getOrCreate()
spark.sparkContext.addPyFile("s3://rakeshnadukuda-lz-007/deltajarfolder/delta-core_2.12-0.8.0.jar")
from delta import *

appconfig="s3://rakeshnadukuda-lz-007/configfiles/appconfiguration.json"
configdata=spark.sparkContext.textFile(appconfig).collect()
data=''.join(configdata)
Data=json.loads(data)

def getfiles():
    l = []
    s3 = boto3.resource('s3')
    bucket = s3.Bucket(name="rakeshnadukuda-lz-007")
    files=[]
    for my_bucket_object in bucket.objects.filter(Prefix="datasets/"):
        z=list(my_bucket_object.key.split("/"))
        files.append(z[1])
    dataset = []
    for i in range(1,len(files)):
        dataset.append(files[i])
    return dataset

class Actives:
    def __init__(self):
        
        #read actives data  from rawzone and masking the columns,transform the columns,partition the columns
        self.actives_location=Data['maskedcolumns-actives']['source']['data-location']
        self.actives_fileformat = Data['maskedcolumns-actives']['source']['file-format']
        self.transform_actives_columns = Data['maskedcolumns-actives']['transformation-columns']
        self.partition_actives_columns = Data['maskedcolumns-actives']['partition-columns']
        self.masking_actives_columns = Data['maskedcolumns-actives']['masking-columns']
        
        #raw to staging
        self.actives_destination= Data['maskedcolumns-actives']['destination']['data-location']
        self.activesdest_fileformat = Data['maskedcolumns-actives']['destination']['file-format']
        
        
    def write_to_destination(self,df,path,format,partition_columns=[]):
        if partition_columns:
            df.write.mode("overwrite").partitionBy(partition_columns[0],partition_columns[1]).parquet(path)
        else:
            df.write.mode("overwrite").parquet(path)
            
    def transforming(self,df,castingcolumns_dict):
        items=[]
        for item in castingcolumns_dict.keys():
            items.append(item)
        for column in items:
            if castingcolumns_dict[column].split(",")[0]=="DecimalType":
                df=df.withColumn(column,df[column].cast(DecimalType(scale=int(castingcolumns_dict[column].split(",")[1]))))
            elif castingcolumns_dict[column]=="ArrayType-StringType":
                df=df.withColumn(column,f.concat_ws(",",'city','state','location_category'))
        return df
    
    def masking(self,df,columns_list):
        for column in columns_list:
            df = df.withColumn("masked_"+column,f.sha2(f.col(column),256))
        return df
    
    def lookup_function_actives(self, df):
        lookup_location = Data['lookup-dataset']['data-location']
        pii_cols = Data['lookup-dataset']['pii-columns']
        Dataset_Name = 'lookup_table'
        s= df.withColumn("start_date", f.current_date())
        s = s.withColumn("end_date", f.lit("0/0/0000"))
        s = s.withColumn("Flag_active", f.lit("true"))
        source_columns = ['advertising_id','user_id','masked_advertising_id','masked_user_id','start_date','end_date']
        s= s.select(*source_columns)
        
        
        
        try:
            t= DeltaTable.forPath(spark, lookup_location+"/Actives")
            delta_df = t.toDF()
        except pyspark.sql.utils.AnalysisException:
            print('Table does not exist')
            s= s.withColumn("flag_active",f.lit("true"))
            s.write.format("delta").mode("overwrite").save(lookup_location+"/Actives")
            print('Table Created Sucessfully!')
            t = DeltaTable.forPath(spark,lookup_location+"/Actives")
            delta_df = t.toDF()
            delta_df.show(100)
        insertvalues_dict = {}
        for i in source_columns:
            insertvalues_dict[i] = "updates." + i
        insertvalues_dict['start_date'] = f.current_date()
        insertvalues_dict['Flag_active'] = "True"
        insertvalues_dict['end_date'] = "0/0/0000"
        print(insertvalues_dict)
        
        
        actives_condition= 'lookup_table' + ".Flag_Active == true AND " + " OR ".join(["updates." + i + " <> " + 'lookup_table' + "." + i for i in [x for x in source_columns if x.startswith("masked_")]])
        print(actives_condition)
        
        actives_column = ",".join([Dataset_Name + "." + i for i in [x for x in pii_cols]])
        print(actives_column)
        
        updatedColumnsToInsert = s.alias("updates").join(t.toDF().alias(Dataset_Name), pii_cols).where(actives_condition)
        stagedUpdates = (
            updatedColumnsToInsert.selectExpr('NULL as mergeKey', *[f"updates.{i}" for i in s.columns]).union(
                s.selectExpr("concat(" + ','.join([x for x in pii_cols]) + ") as mergeKey", "*")))
        t.alias(Dataset_Name).merge(stagedUpdates.alias("updates"), "concat(" + str(actives_column) + ") = mergeKey").whenMatchedUpdate(
            condition=actives_condition,
            set={  
                "end_date": f.current_date(),
                "Flag_active":"False"
            }
        ).whenNotMatchedInsert(values=insertvalues_dict).execute()
        
        
        
        for i in pii_cols:
            df = df.drop(i).withColumnRenamed("masked_" + i, i)
        return df

class ViewerShip:
    def __init__(self):
        #read data from rawzone and masking columns,transformation columns,partition the columns
        
        self.viewers_location=Data['maskedcolumns-viewership']['source']['data-location']
        self.masking_viewers_columns = Data['maskedcolumns-viewership']['source']['file-format']
        self.transform_viewers_columns = Data['maskedcolumns-viewership']['transformation-columns']
        self.partition_viewers_columns = Data['maskedcolumns-viewership']['partition-columns']
        self.masked_viewers_columns = Data['maskedcolumns-viewership']['masking-columns']
        
        
        #move data to stagining zone
        self.viewers_destination= Data['maskedcolumns-viewership']['destination']['data-location']
        self.viewersdest_fileformat=Data['maskedcolumns-viewership']['destination']['file-format']
        
    def lookup_function_viewers(self, df):
        lookup_location = Data['lookup-dataset']['data-location']
        pii_cols = Data['lookup-dataset']['pii-columns']
        Dataset_Name = 'lookup_table'
        s= df.withColumn("start_date", f.current_date())
        s = s.withColumn("end_date", f.lit("0/0/0000"))
        s = s.withColumn("Flag_active", f.lit("true"))
        pii_cols = [i for i in pii_cols if i in s.columns]
        source_columns = ['advertising_id','masked_advertising_id','start_date','end_date']
        s= s.select(*source_columns)
        try:
            t= DeltaTable.forPath(spark, lookup_location+"/Viewership")
            delta_df = t.toDF()
        except pyspark.sql.utils.AnalysisException:
            print('Table does not exist')
            s= s.withColumn("flag_active",f.lit("true"))
            s.write.format("delta").mode("overwrite").save(lookup_location+"/Viewership")
            print('Table Created Sucessfully!')
            t = DeltaTable.forPath(spark,lookup_location+"/Viewership")
            delta_df = t.toDF()
            delta_df.show(100)
        insertvalues_dict = {}
        for i in source_columns:
            insertvalues_dict[i] = "updates." + i
        insertvalues_dict['start_date'] = f.current_date()
        insertvalues_dict['Flag_active'] = "True"
        insertvalues_dict['end_date'] = "0/0/0000"
        print(insertvalues_dict)
        
        
        viewers_condition = 'lookup_table' + ".Flag_Active == true AND " + " OR ".join(["updates." + i + " <> " + 'lookup_table' + "." + i for i in [x for x in source_columns if x.startswith("masked_")]])
        print(viewers_condition)
        
        viewers_column= ",".join([Dataset_Name + "." + i for i in [x for x in pii_cols]])
        print(viewers_column)
        
        updatedColumnsToInsert = s.alias("updates").join(t.toDF().alias(Dataset_Name), pii_cols).where(viewers_condition)
        stagedUpdates = (
            updatedColumnsToInsert.selectExpr('NULL as mergeKey', *[f"updates.{i}" for i in s.columns]).union(
                s.selectExpr("concat(" + ','.join([x for x in pii_cols]) + ") as mergeKey", "*")))
        t.alias(Dataset_Name).merge(stagedUpdates.alias("updates"), "concat(" + str(viewers_column) + ") = mergeKey").whenMatchedUpdate(
            condition=viewers_condition,
            set={  
                "end_date": f.current_date(),
                "Flag_active":"False"
            }
        ).whenNotMatchedInsert(values=insertvalues_dict).execute()
        
        
        
        for i in pii_cols:
            df = df.drop(i).withColumnRenamed("masked_" + i, i)
        return df

a=Actives()
v=ViewerShip()
location=Data['maskedcolumns-actives']['source']['data-location']
s="/".join(location.split("/")[0:-1])
objects = getfiles()
if 'Actives.parquet' in objects:
    actives=s+"/"+"ActivesAirflow"+"/"
    actives_raw =spark.read.parquet(actives)
    print(a.transform_actives_columns,a.partition_actives_columns,a.masking_actives_columns )
    cast_actives=a.transforming(actives_raw,a.transform_actives_columns)
    masked_actives=a.masking(cast_actives,a.masking_actives_columns )
    actives_data = a.lookup_function_actives(masked_actives)
    a.write_to_destination(actives_data,a.actives_destination,a.activesdest_fileformat,a.partition_actives_columns)
if 'Viewership.parquet' in objects:
    viewers=s+"/"+"ViewershipAirflow"+"/"
    viewers_raw =spark.read.parquet(viewers)
    print(v.transform_viewers_columns,v.partition_viewers_columns,v.masking_viewers_columns )
    cast_viewers=a.transforming(viewers_raw,v.transform_viewers_columns)
    masked_viewers=a.masking(cast_viewers,v.masked_viewers_columns )
    viewers_data = v.lookup_function_viewers(masked_viewers)
    a.write_to_destination(viewers_data,v.viewers_destination,v.viewersdest_fileformat,v.partition_viewers_columns)



